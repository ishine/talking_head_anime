{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "10e2a954",
   "metadata": {},
   "source": [
    "# Talking Head Anime from a Single Image (Manual Poser Tool)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46280224",
   "metadata": {},
   "source": [
    "Copycat of [original talking head anime](https://github.com/pkhungurn/talking-head-anime-demo/blob/master/tha_colab.ipynb), local ipynb inference version."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d67a4436",
   "metadata": {},
   "source": [
    "### Instruction\n",
    "1. Edit var `path_ckpt` to your ckpt path.\n",
    "2. If GPU is usable, change device to `cuda` or whatever you want.\n",
    "3. Run the cells below, one by one and then play with the GUI."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79b36832",
   "metadata": {},
   "source": [
    "### Input Images\n",
    "\n",
    "1. Must be an image of a single humanoid anime character.\n",
    "2. Must have an alpha channel, where background pixels must have color value of RGBA=(0, 0, 0, 0).\n",
    "3. Input image is recommended to have closed mouth, face in the middle of the image, human-like character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e98a18cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cpu'\n",
    "path_morpher_ckpt = 'checkpoint/step_163548.pth'\n",
    "path_rotator_ckpt = 'checkpoint/step_148680.pth'\n",
    "imsize = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5ba1fa39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import IPython.display as ipd\n",
    "\n",
    "from models.tha1 import FaceMorpher, TwoAlgorithmFaceRotator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "64d3b7b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets\n",
    "import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "011ffd28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_pytorch_image(pytorch_image, output_widget=None):\n",
    "    np_image = (255 * pytorch_image.detach().cpu().permute((1, 2, 0)).numpy()).astype(np.uint8)\n",
    "    np_image = np.clip(np_image, 0, 255)\n",
    "    ipd.display(Image.fromarray(np_image))\n",
    "    \n",
    "def extract_pytorch_image_from_filelike(file):\n",
    "    pil_image = Image.open(file)\n",
    "    im = np.asarray(pil_image)\n",
    "    \n",
    "#     im = cv2.imread(file, cv2.IMREAD_UNCHANGED)\n",
    "    im = cv2.resize(im, (imsize, imsize))\n",
    "#     im = cv2.cvtColor(im, cv2.COLOR_BGRA2RGBA)\n",
    "    im = torch.from_numpy(im).permute((2, 0, 1)) / 255.\n",
    "    return im"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a44853b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "last_torch_input_image = None\n",
    "torch_input_image = None\n",
    "\n",
    "# image widgets, upload button\n",
    "input_image_widget = ipywidgets.Output(\n",
    "    layout={\n",
    "        'border': '1px solid black',\n",
    "        'width': '256px',\n",
    "        'height': '256px'\n",
    "    })\n",
    "\n",
    "output_image_widget = ipywidgets.Output(\n",
    "    layout={\n",
    "        'border': '1px solid black',\n",
    "        'width': '256px',\n",
    "        'height': '256px'\n",
    "    }\n",
    ")\n",
    "\n",
    "upload_input_image_button = ipywidgets.FileUpload(\n",
    "    accept='.png',\n",
    "    multiple=False,\n",
    "    layout={\n",
    "        'width': '256px'\n",
    "    }\n",
    ")\n",
    "\n",
    "\n",
    "# control sliders\n",
    "eye_left_slider = ipywidgets.FloatSlider(\n",
    "    value=0.0,\n",
    "    min=0.0,\n",
    "    max=1.0,\n",
    "    step=0.01,\n",
    "    description=\"Left Eye:\",\n",
    "    readout=True,\n",
    "    readout_format=\".2f\"\n",
    ")\n",
    "eye_right_slider = ipywidgets.FloatSlider(\n",
    "    value=0.0,\n",
    "    min=0.0,\n",
    "    max=1.0,\n",
    "    step=0.01,\n",
    "    description=\"Right Eye:\",\n",
    "    readout=True,\n",
    "    readout_format=\".2f\"\n",
    ")\n",
    "mouth_slider = ipywidgets.FloatSlider(\n",
    "    value=0.0,\n",
    "    min=0.0,\n",
    "    max=1.0,\n",
    "    step=0.01,\n",
    "    description=\"Mouth:\",\n",
    "    readout=True,\n",
    "    readout_format=\".2f\"\n",
    ")\n",
    "\n",
    "head_x_slider = ipywidgets.FloatSlider(\n",
    "    value=0.0,\n",
    "    min=-30,\n",
    "    max=30,\n",
    "    step=1,\n",
    "    description=\"X-axis:\",\n",
    "    readout=True,\n",
    "    readout_format=\".2f\"\n",
    ")\n",
    "head_y_slider = ipywidgets.FloatSlider(\n",
    "    value=0.0,\n",
    "    min=-30,\n",
    "    max=30,\n",
    "    step=1,\n",
    "    description=\"Y-axis:\",\n",
    "    readout=True,\n",
    "    readout_format=\".2f\",    \n",
    ")\n",
    "neck_z_slider = ipywidgets.FloatSlider(\n",
    "    value=0.0,\n",
    "    min=-30,\n",
    "    max=30,\n",
    "    step=1,\n",
    "    description=\"Z-axis:\",\n",
    "    readout=True,\n",
    "    readout_format=\".2f\",    \n",
    ")\n",
    "\n",
    "\n",
    "# control panels\n",
    "control_panel = ipywidgets.VBox([    \n",
    "    ipywidgets.HTML(value=\"<center><b>Head Rotation</b></center>\"),\n",
    "    head_x_slider,\n",
    "    head_y_slider,\n",
    "    neck_z_slider,\n",
    "    ipywidgets.HTML(value=\"<hr>\"),\n",
    "    ipywidgets.HTML(value=\"<center><b>Facial Features</b></center>\"),\n",
    "    eye_left_slider,\n",
    "    eye_right_slider,\n",
    "    mouth_slider,\n",
    "])\n",
    "\n",
    "controls = ipywidgets.HBox([\n",
    "    ipywidgets.VBox([\n",
    "        input_image_widget, \n",
    "        upload_input_image_button\n",
    "    ]),\n",
    "    control_panel,\n",
    "    ipywidgets.HTML(value=\"&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\"),\n",
    "    output_image_widget,\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8192feee",
   "metadata": {},
   "outputs": [],
   "source": [
    "morpher = FaceMorpher(None)\n",
    "morpher.load_state_dict(torch.load(path_morpher_ckpt, map_location='cpu')['FaceMorpher']['state_dict'])\n",
    "morpher = morpher.to(device)\n",
    "morpher = morpher.eval()\n",
    "\n",
    "rotator = TwoAlgorithmFaceRotator(None)\n",
    "rotator.load_state_dict(torch.load(path_rotator_ckpt, map_location='cpu')['FaceRotator']['state_dict'])\n",
    "rotator = rotator.to(device)\n",
    "rotator = rotator.eval()\n",
    "\n",
    "# pose vector\n",
    "pose_size = 6\n",
    "last_pose = torch.zeros(1, pose_size).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4ec448e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pose():\n",
    "    pose = torch.zeros(1, pose_size)\n",
    "    \n",
    "    pose[0, 0] = head_x_slider.value\n",
    "    pose[0, 1] = head_y_slider.value\n",
    "    pose[0, 2] = neck_z_slider.value\n",
    "    \n",
    "    pose[0, 3] = mouth_slider.value\n",
    "    pose[0, 4] = eye_left_slider.value\n",
    "    pose[0, 5] = eye_right_slider.value\n",
    "        \n",
    "    return pose.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "73aa2c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update(change):\n",
    "    global last_pose\n",
    "    global last_torch_input_image\n",
    "        \n",
    "    if torch_input_image is None:\n",
    "        return\n",
    "        \n",
    "    needs_update = False\n",
    "    if last_torch_input_image is None:\n",
    "        needs_update = True        \n",
    "    else:\n",
    "        if (torch_input_image - last_torch_input_image).abs().max().item() > 0:\n",
    "            needs_update = True         \n",
    "            \n",
    "    pose = get_pose()\n",
    "    if (pose - last_pose).abs().max().item() > 0:\n",
    "        needs_update = True\n",
    "    \n",
    "    if not needs_update:\n",
    "        return\n",
    "   \n",
    "    output_image = rotator(morpher(torch_input_image, pose[:, 3:])['e2'], pose[:, :3])['e4'][0]\n",
    "    with output_image_widget:\n",
    "        output_image_widget.clear_output(wait=True)\n",
    "        show_pytorch_image(output_image, output_image_widget)  \n",
    "        \n",
    "    last_torch_input_image = torch_input_image\n",
    "    last_pose = pose\n",
    "        \n",
    "def upload_image(change):\n",
    "    global torch_input_image\n",
    "    for name, file_info in upload_input_image_button.value.items():\n",
    "        torch_input_image = extract_pytorch_image_from_filelike(io.BytesIO(file_info['content'])).to(device)\n",
    "        torch_input_image = torch_input_image.unsqueeze(0)\n",
    "    if torch_input_image is not None:\n",
    "        n,c,h,w = torch_input_image.shape\n",
    "        if h != imsize or w != imsize:\n",
    "            with input_image_widget:\n",
    "                input_image_widget.clear_output(wait=True)\n",
    "                display(ipywidgets.HTML(f\"Image must be {imsize}x{imsize} in size!!!\"))\n",
    "            torch_input_image = None\n",
    "        if c != 4:\n",
    "            with input_image_widget:\n",
    "                input_image_widget.clear_output(wait=True)\n",
    "                display(ipywidgets.HTML(\"Image must have an alpha channel!!!\"))                \n",
    "            torch_input_image = None\n",
    "        if torch_input_image is not None:\n",
    "            with input_image_widget:\n",
    "                input_image_widget.clear_output(wait=True)\n",
    "                show_pytorch_image(torch_input_image[0], input_image_widget)\n",
    "        update(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "57690421",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5138db6bb7f44b0a892a5f295bd5c9f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(VBox(children=(Output(layout=Layout(border='1px solid black', height='256px', width='256px')), â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mindslab\\miniconda3\\envs\\blender_py37\\lib\\site-packages\\torch\\functional.py:445: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ..\\aten\\src\\ATen\\native\\TensorShape.cpp:2157.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
      "C:\\Users\\mindslab\\miniconda3\\envs\\blender_py37\\lib\\site-packages\\torch\\nn\\functional.py:4004: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.\n",
      "  \"Default grid_sample and affine_grid behavior has changed \"\n"
     ]
    }
   ],
   "source": [
    "display(controls)\n",
    "upload_input_image_button.observe(upload_image, names='value')\n",
    "eye_left_slider.observe(update, 'value')\n",
    "eye_right_slider.observe(update, 'value')\n",
    "mouth_slider.observe(update, 'value')\n",
    "head_x_slider.observe(update, 'value')\n",
    "head_y_slider.observe(update, 'value')\n",
    "neck_z_slider.observe(update, 'value')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
